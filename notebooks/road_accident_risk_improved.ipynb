{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0eacc0d",
   "metadata": {},
   "source": [
    "# Improved Road Accident Risk Prediction\n",
    "\n",
    "**Competition:** Kaggle Playground Series - Season 5, Episode 10\n",
    "\n",
    "**Goal:** Predict accident risk (0-1) for different road types\n",
    "\n",
    "**Evaluation Metric:** RMSE (Root Mean Squared Error)\n",
    "\n",
    "**Target Score:** < 0.05 RMSE\n",
    "\n",
    "## Strategy\n",
    "1. Enhanced Feature Engineering\n",
    "2. Advanced Model Stacking\n",
    "3. 5-Fold Cross-Validation\n",
    "4. Weighted Averaging of Predictions\n",
    "5. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('✓ Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8369698",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "DATA_DIR = Path('../playground-series-s5e10')\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH = DATA_DIR / 'test.csv'\n",
    "SUBMISSION_PATH = DATA_DIR / 'sample_submission.csv'\n",
    "\n",
    "# Load data\n",
    "print('Loading data...')\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "sample_submission = pd.read_csv(SUBMISSION_PATH)\n",
    "\n",
    "print(f'Train shape: {train_df.shape}')\n",
    "print(f'Test shape: {test_df.shape}')\n",
    "print(f'\\nFirst few rows:')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data overview\n",
    "print('Column types:')\n",
    "print(train_df.dtypes)\n",
    "print(f'\\nTarget statistics:')\n",
    "print(train_df['accident_risk'].describe())\n",
    "print(f'\\nMissing values:')\n",
    "print(train_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12120a4a",
   "metadata": {},
   "source": [
    "## 2. Enhanced Feature Engineering\n",
    "\n",
    "Creating advanced features to capture complex patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Original features from utils\n",
    "    df['speed_per_lane'] = df['speed_limit'] / (df['num_lanes'] + 0.001)  # Adding small value to avoid division by zero\n",
    "    df['curvature_speed'] = df['curvature'] * df['speed_limit']\n",
    "    df['accidents_per_lane'] = df['num_reported_accidents'] / (df['num_lanes'] + 0.001)\n",
    "    \n",
    "    # Boolean combinations\n",
    "    df['risky_conditions'] = (\n",
    "        (df['weather'] == 'rainy') & \n",
    "        (df['lighting'] == 'dim')\n",
    "    ).astype(int)\n",
    "    \n",
    "    df['peak_holiday'] = (\n",
    "        (df['holiday'] == True) & \n",
    "        (df['time_of_day'].isin(['evening', 'afternoon']))\n",
    "    ).astype(int)\n",
    "    \n",
    "    df['school_morning'] = (\n",
    "        (df['school_season'] == True) & \n",
    "        (df['time_of_day'] == 'morning')\n",
    "    ).astype(int)\n",
    "    \n",
    "    df['high_speed_curve'] = (\n",
    "        (df['speed_limit'] > 50) & \n",
    "        (df['curvature'] > 0.5)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df['no_signs_bad_weather'] = (\n",
    "        (df['road_signs_present'] == False) & \n",
    "        (df['weather'] != 'clear')\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Polynomial features\n",
    "    df['curvature_squared'] = df['curvature'] ** 2\n",
    "    df['speed_squared'] = df['speed_limit'] ** 2\n",
    "    df['accidents_squared'] = df['num_reported_accidents'] ** 2\n",
    "    \n",
    "    # Additional enhanced features\n",
    "    df['curvature_per_lane'] = df['curvature'] / (df['num_lanes'] + 0.001)\n",
    "    df['speed_curvature_ratio'] = df['speed_limit'] / (df['curvature'] + 0.001)\n",
    "    df['lighting_weather'] = df['lighting'] + '_' + df['weather']\n",
    "    \n",
    "    # Risk factors\n",
    "    df['high_risk_combo'] = (\n",
    "        (df['weather'] != 'clear') & \n",
    "        (df['lighting'] != 'daylight') & \n",
    "        (df['curvature'] > 0.5)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Time-based features\n",
    "    time_mapping = {'morning': 0, 'afternoon': 1, 'evening': 2, 'night': 3}\n",
    "    df['time_numeric'] = df['time_of_day'].map(time_mapping)\n",
    "    \n",
    "    # Binned features\n",
    "    df['speed_category'] = pd.cut(\n",
    "        df['speed_limit'], \n",
    "        bins=[0, 35, 50, 70], \n",
    "        labels=['low', 'medium', 'high']\n",
    "    )\n",
    "    \n",
    "    df['curvature_category'] = pd.cut(\n",
    "        df['curvature'], \n",
    "        bins=[0, 0.33, 0.66, 1.0], \n",
    "        labels=['straight', 'moderate', 'sharp']\n",
    "    )\n",
    "    \n",
    "    # Combined categorical features\n",
    "    df['road_weather'] = df['road_type'] + '_' + df['weather']\n",
    "    df['road_lighting'] = df['road_type'] + '_' + df['lighting']\n",
    "    df['weather_time'] = df['weather'] + '_' + df['time_of_day']\n",
    "    \n",
    "    # Ensure all categorical columns are strings and handle NaN\n",
    "    cat_cols_to_convert = ['speed_category', 'curvature_category', 'road_weather', 'road_lighting', 'weather_time']\n",
    "    for col in cat_cols_to_convert:\n",
    "        df[col] = df[col].astype(str).fillna('unknown')\n",
    "    \n",
    "    # Interaction features\n",
    "    df['lane_speed_interaction'] = df['num_lanes'] * df['speed_limit']\n",
    "    df['weather_lighting'] = df['weather'] + '_' + df['lighting']\n",
    "    \n",
    "    # Statistical aggregations\n",
    "    df['accidents_log'] = np.log1p(df['num_reported_accidents'])\n",
    "    df['curvature_log'] = np.log1p(df['curvature'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('Creating enhanced features...')\n",
    "train_df = create_enhanced_features(train_df)\n",
    "test_df = create_enhanced_features(test_df)\n",
    "print('✓ Enhanced features created!')\n",
    "print(f'New train shape: {train_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0850a9",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature types\n",
    "TARGET = 'accident_risk'\n",
    "ID_COL = 'id'\n",
    "\n",
    "# Categorical features\n",
    "CATEGORICAL_COLS = [\n",
    "    'road_type', 'lighting', 'weather', 'road_signs_present',\n",
    "    'public_road', 'time_of_day', 'holiday', 'school_season',\n",
    "    'speed_category', 'curvature_category',\n",
    "    'road_weather', 'road_lighting', 'weather_time', 'lighting_weather', 'weather_lighting'\n",
    "]\n",
    "\n",
    "# Get all feature columns (excluding id and target)\n",
    "FEATURE_COLS = [col for col in train_df.columns if col not in [ID_COL, TARGET]]\n",
    "\n",
    "print(f'Total features: {len(FEATURE_COLS)}')\n",
    "print(f'Categorical features: {len(CATEGORICAL_COLS)}')\n",
    "print(f'Numerical features: {len(FEATURE_COLS) - len(CATEGORICAL_COLS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features for XGBoost and LightGBM\n",
    "def encode_categoricals(train, test, cat_cols):\n",
    "    \"\"\"Label encode categorical features.\"\"\"\n",
    "    train_encoded = train.copy()\n",
    "    test_encoded = test.copy()\n",
    "    \n",
    "    encoders = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        train_encoded[col] = le.fit_transform(train[col].astype(str))\n",
    "        test_encoded[col] = le.transform(test[col].astype(str))\n",
    "        encoders[col] = le\n",
    "    \n",
    "    return train_encoded, test_encoded, encoders\n",
    "\n",
    "# Prepare datasets\n",
    "X = train_df[FEATURE_COLS].copy()\n",
    "y = train_df[TARGET].copy()\n",
    "X_test = test_df[FEATURE_COLS].copy()\n",
    "\n",
    "# Encoded version for XGBoost/LightGBM\n",
    "X_encoded, X_test_encoded, encoders = encode_categoricals(X, X_test, CATEGORICAL_COLS)\n",
    "\n",
    "print('✓ Data prepared for modeling!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a5e24",
   "metadata": {},
   "source": [
    "## 4. Model Training with Cross-Validation\n",
    "\n",
    "Training multiple models with 5-fold CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f336d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Storage for predictions\n",
    "oof_catboost = np.zeros(len(X))\n",
    "oof_xgboost = np.zeros(len(X))\n",
    "oof_lightgbm = np.zeros(len(X))\n",
    "\n",
    "test_catboost = np.zeros(len(X_test))\n",
    "test_xgboost = np.zeros(len(X_test))\n",
    "test_lightgbm = np.zeros(len(X_test))\n",
    "\n",
    "fold_scores = {'catboost': [], 'xgboost': [], 'lightgbm': []}\n",
    "\n",
    "print('Starting cross-validation training...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed569b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with progress tracking\n",
    "for fold, (train_idx, valid_idx) in enumerate(tqdm(list(kfold.split(X)), desc='Folds'), 1):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Fold {fold}/{N_FOLDS}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    X_train_enc, X_valid_enc = X_encoded.iloc[train_idx], X_encoded.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    # ===== CatBoost =====\n",
    "    print('\\n[1/3] Training CatBoost...')\n",
    "    cat_features = [X_train.columns.get_loc(col) for col in CATEGORICAL_COLS]\n",
    "    \n",
    "    # Ensure no NaN values in categorical features for CatBoost\n",
    "    X_train_cb = X_train.fillna('unknown')\n",
    "    X_valid_cb = X_valid.fillna('unknown')\n",
    "    X_test_cb = X_test.fillna('unknown')\n",
    "    \n",
    "    train_pool = Pool(X_train_cb, y_train, cat_features=cat_features)\n",
    "    valid_pool = Pool(X_valid_cb, y_valid, cat_features=cat_features)\n",
    "    test_pool = Pool(X_test_cb, cat_features=cat_features)\n",
    "    \n",
    "    cb_model = CatBoostRegressor(\n",
    "        iterations=1500,  # Reduced from 3000 to prevent overfitting\n",
    "        learning_rate=0.05,  # Increased learning rate with fewer iterations\n",
    "        depth=6,  # Reduced depth\n",
    "        loss_function='RMSE',\n",
    "        eval_metric='RMSE',\n",
    "        subsample=0.8,\n",
    "        colsample_bylevel=0.8,\n",
    "        random_seed=RANDOM_STATE + fold,\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    cb_model.fit(\n",
    "        train_pool,\n",
    "        eval_set=valid_pool,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    oof_catboost[valid_idx] = cb_model.predict(valid_pool)\n",
    "    test_catboost += cb_model.predict(test_pool) / N_FOLDS\n",
    "    \n",
    "    cb_score = mean_squared_error(y_valid, oof_catboost[valid_idx], squared=False)\n",
    "    fold_scores['catboost'].append(cb_score)\n",
    "    print(f'  CatBoost RMSE: {cb_score:.5f}')\n",
    "    \n",
    "    # ===== XGBoost =====\n",
    "    print('\\n[2/3] Training XGBoost...')\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=RANDOM_STATE + fold,\n",
    "        tree_method='hist',\n",
    "        enable_categorical=True,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(\n",
    "        X_train_enc, y_train,\n",
    "        eval_set=[(X_valid_enc, y_valid)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    oof_xgboost[valid_idx] = xgb_model.predict(X_valid_enc)\n",
    "    test_xgboost += xgb_model.predict(X_test_encoded) / N_FOLDS\n",
    "    \n",
    "    xgb_score = mean_squared_error(y_valid, oof_xgboost[valid_idx], squared=False)\n",
    "    fold_scores['xgboost'].append(xgb_score)\n",
    "    print(f'  XGBoost RMSE: {xgb_score:.5f}')\n",
    "    \n",
    "    # ===== LightGBM =====\n",
    "    print('\\n[3/3] Training LightGBM...')\n",
    "    lgb_model = LGBMRegressor(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=48,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=RANDOM_STATE + fold,\n",
    "        verbose=-1,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(\n",
    "        X_train_enc, y_train,\n",
    "        eval_set=[(X_valid_enc, y_valid)],\n",
    "        callbacks=[early_stopping(100), log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    oof_lightgbm[valid_idx] = lgb_model.predict(X_valid_enc)\n",
    "    test_lightgbm += lgb_model.predict(X_test_encoded) / N_FOLDS\n",
    "    \n",
    "    lgb_score = mean_squared_error(y_valid, oof_lightgbm[valid_idx], squared=False)\n",
    "    fold_scores['lightgbm'].append(lgb_score)\n",
    "    print(f'  LightGBM RMSE: {lgb_score:.5f}')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print('Training complete!')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd2d3f",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62818b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CV scores\n",
    "cv_catboost = mean_squared_error(y, oof_catboost, squared=False)\n",
    "cv_xgboost = mean_squared_error(y, oof_xgboost, squared=False)\n",
    "cv_lightgbm = mean_squared_error(y, oof_lightgbm, squared=False)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('CROSS-VALIDATION RESULTS')\n",
    "print('='*60)\n",
    "print(f'CatBoost  CV RMSE: {cv_catboost:.5f}')\n",
    "print(f'XGBoost   CV RMSE: {cv_xgboost:.5f}')\n",
    "print(f'LightGBM  CV RMSE: {cv_lightgbm:.5f}')\n",
    "print('='*60)\n",
    "\n",
    "# Fold-wise scores\n",
    "print('\\nFold-wise scores:')\n",
    "fold_df = pd.DataFrame(fold_scores)\n",
    "fold_df.index = [f'Fold {i+1}' for i in range(N_FOLDS)]\n",
    "fold_df.loc['Mean'] = fold_df.mean()\n",
    "fold_df.loc['Std'] = fold_df.std()\n",
    "print(fold_df.round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV scores\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "fold_df.iloc[:-2].plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Cross-Validation RMSE by Fold', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Fold', fontsize=12)\n",
    "ax.set_ylabel('RMSE', fontsize=12)\n",
    "ax.legend(title='Model', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390128b",
   "metadata": {},
   "source": [
    "## 6. Ensemble Predictions\n",
    "\n",
    "Combining predictions from all three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81414dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different ensemble weights\n",
    "print('Testing ensemble weights...\\n')\n",
    "\n",
    "best_score = float('inf')\n",
    "best_weights = None\n",
    "\n",
    "# Test different weight combinations\n",
    "weight_options = [\n",
    "    (0.4, 0.3, 0.3),  # Equal-ish\n",
    "    (0.5, 0.25, 0.25),  # Favor CatBoost\n",
    "    (0.34, 0.33, 0.33),  # Exactly equal\n",
    "    (0.6, 0.2, 0.2),  # Heavy CatBoost\n",
    "    (0.45, 0.35, 0.2),  # Favor top 2\n",
    "    (0.4, 0.4, 0.2),  # CatBoost + XGBoost heavy\n",
    "    (0.35, 0.35, 0.3),  # Close to equal\n",
    "]\n",
    "\n",
    "for weights in weight_options:\n",
    "    w_cb, w_xgb, w_lgb = weights\n",
    "    \n",
    "    oof_ensemble = (\n",
    "        w_cb * oof_catboost +\n",
    "        w_xgb * oof_xgboost +\n",
    "        w_lgb * oof_lightgbm\n",
    "    )\n",
    "    \n",
    "    ensemble_score = mean_squared_error(y, oof_ensemble, squared=False)\n",
    "    \n",
    "    print(f'Weights {weights}: RMSE = {ensemble_score:.5f}')\n",
    "    \n",
    "    if ensemble_score < best_score:\n",
    "        best_score = ensemble_score\n",
    "        best_weights = weights\n",
    "\n",
    "print(f'\\n✓ Best weights: {best_weights}')\n",
    "print(f'✓ Best ensemble CV RMSE: {best_score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9495ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final ensemble predictions\n",
    "w_cb, w_xgb, w_lgb = best_weights\n",
    "\n",
    "test_ensemble = (\n",
    "    w_cb * test_catboost +\n",
    "    w_xgb * test_xgboost +\n",
    "    w_lgb * test_lightgbm\n",
    ")\n",
    "\n",
    "# Clip predictions to valid range [0, 1]\n",
    "test_ensemble = np.clip(test_ensemble, 0, 1)\n",
    "\n",
    "print(f'Final test predictions:')\n",
    "print(f'  Min: {test_ensemble.min():.4f}')\n",
    "print(f'  Max: {test_ensemble.max():.4f}')\n",
    "print(f'  Mean: {test_ensemble.mean():.4f}')\n",
    "print(f'  Median: {np.median(test_ensemble):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f2743",
   "metadata": {},
   "source": [
    "## 7. Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b536df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submission = sample_submission.copy()\n",
    "submission['accident_risk'] = test_ensemble\n",
    "\n",
    "# Save to file\n",
    "output_path = Path('../submission_improved.csv')\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print('✓ Submission file created successfully!')\n",
    "print(f'  Location: {output_path.resolve()}')\n",
    "print(f'  Shape: {submission.shape}')\n",
    "print(f'\\nFirst few rows:')\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e782ba9",
   "metadata": {},
   "source": [
    "## 8. Analysis & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a912c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training target distribution\n",
    "axes[0, 0].hist(y, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].set_title('Training Target Distribution', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Accident Risk')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Test predictions distribution\n",
    "axes[0, 1].hist(test_ensemble, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 1].set_title('Test Predictions Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Predicted Accident Risk')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# OOF vs Actual scatter\n",
    "oof_ensemble = (\n",
    "    w_cb * oof_catboost +\n",
    "    w_xgb * oof_xgboost +\n",
    "    w_lgb * oof_lightgbm\n",
    ")\n",
    "axes[1, 0].scatter(y, oof_ensemble, alpha=0.3, s=1)\n",
    "axes[1, 0].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "axes[1, 0].set_title('OOF Predictions vs Actual', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Actual Accident Risk')\n",
    "axes[1, 0].set_ylabel('Predicted Accident Risk')\n",
    "\n",
    "# Model comparison\n",
    "model_scores = pd.Series({\n",
    "    'CatBoost': cv_catboost,\n",
    "    'XGBoost': cv_xgboost,\n",
    "    'LightGBM': cv_lightgbm,\n",
    "    'Ensemble': best_score\n",
    "})\n",
    "model_scores.plot(kind='bar', ax=axes[1, 1], color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])\n",
    "axes[1, 1].set_title('Model Performance Comparison', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('CV RMSE')\n",
    "axes[1, 1].set_xlabel('Model')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals = y - oof_ensemble\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residual distribution\n",
    "axes[0].hist(residuals, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[0].set_title('Residual Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Residual (Actual - Predicted)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Residual scatter\n",
    "axes[1].scatter(oof_ensemble, residuals, alpha=0.3, s=1)\n",
    "axes[1].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_title('Residual Plot', fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Accident Risk')\n",
    "axes[1].set_ylabel('Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Residual statistics:')\n",
    "print(f'  Mean: {residuals.mean():.6f}')\n",
    "print(f'  Std: {residuals.std():.6f}')\n",
    "print(f'  Min: {residuals.min():.6f}')\n",
    "print(f'  Max: {residuals.max():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b8762",
   "metadata": {},
   "source": [
    "## 9. Advanced Feature Engineering and Stacking\n",
    "\n",
    "Now let's try a more advanced approach with meta-features and stacking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meta-features from model predictions\n",
    "print('Creating meta-features for stacking...')\n",
    "\n",
    "# Create meta-features using out-of-fold predictions\n",
    "meta_features = np.column_stack([oof_catboost, oof_xgboost, oof_lightgbm])\n",
    "meta_test = np.column_stack([test_catboost, test_xgboost, test_lightgbm])\n",
    "\n",
    "# Add some interaction features between model predictions\n",
    "meta_features_interactions = np.column_stack([\n",
    "    oof_catboost * oof_xgboost,\n",
    "    oof_catboost * oof_lightgbm,\n",
    "    oof_xgboost * oof_lightgbm,\n",
    "    (oof_catboost + oof_xgboost + oof_lightgbm) / 3,  # Simple average\n",
    "    np.maximum(oof_catboost, np.maximum(oof_xgboost, oof_lightgbm)),  # Max\n",
    "    np.minimum(oof_catboost, np.minimum(oof_xgboost, oof_lightgbm))   # Min\n",
    "])\n",
    "\n",
    "# Combine base meta-features with interaction features\n",
    "meta_features = np.column_stack([meta_features, meta_features_interactions])\n",
    "\n",
    "# Test interactions\n",
    "meta_test_interactions = np.column_stack([\n",
    "    test_catboost * test_xgboost,\n",
    "    test_catboost * test_lightgbm,\n",
    "    test_xgboost * test_lightgbm,\n",
    "    (test_catboost + test_xgboost + test_lightgbm) / 3,  # Simple average\n",
    "    np.maximum(test_catboost, np.maximum(test_xgboost, test_lightgbm)),  # Max\n",
    "    np.minimum(test_catboost, np.minimum(test_xgboost, test_lightgbm))   # Min\n",
    "])\n",
    "\n",
    "meta_test = np.column_stack([meta_test, meta_test_interactions])\n",
    "\n",
    "print(f'Meta-features shape: {meta_features.shape}')\n",
    "print(f'Meta-test shape: {meta_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a meta-model (level 2 model) using the meta-features\n",
    "print('Training meta-model (stacking)...')\n",
    "\n",
    "# Use cross-validation for the meta-model to avoid overfitting\n",
    "meta_oof = np.zeros(len(y))\n",
    "meta_predictions = np.zeros(len(test_df))\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kfold.split(meta_features)):\n",
    "    X_meta_train, X_meta_valid = meta_features[train_idx], meta_features[valid_idx]\n",
    "    y_meta_train, y_meta_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    # Use a simple ridge regression as the meta-model\n",
    "    meta_model = Ridge(alpha=1.0)\n",
    "    meta_model.fit(X_meta_train, y_meta_train)\n",
    "    \n",
    "    meta_oof[valid_idx] = meta_model.predict(X_meta_valid)\n",
    "    meta_predictions += meta_model.predict(meta_test) / N_FOLDS\n",
    "    \n",
    "    meta_fold_score = mean_squared_error(y_meta_valid, meta_oof[valid_idx], squared=False)\n",
    "    print(f'Meta-model fold {fold+1} RMSE: {meta_fold_score:.5f}')\n",
    "\n",
    "# Calculate final meta-model score\n",
    "meta_score = mean_squared_error(y, meta_oof, squared=False)\n",
    "print(f'\\nMeta-model CV RMSE: {meta_score:.5f}')\n",
    "\n",
    "# Clip the meta predictions\n",
    "meta_predictions_clipped = np.clip(meta_predictions, 0, 1)\n",
    "\n",
    "# Compare with the simple ensemble\n",
    "print(f'Simple ensemble CV RMSE: {best_score:.5f}')\n",
    "print(f'Stacking ensemble CV RMSE: {meta_score:.5f}')\n",
    "\n",
    "if meta_score < best_score:\n",
    "    print('\\n✓ Stacking ensemble performs better!')\n",
    "    final_predictions = meta_predictions_clipped\n",
    "    final_score = meta_score\n",
    "else:\n",
    "    print('\\n✓ Simple ensemble performs better!')\n",
    "    final_predictions = test_ensemble\n",
    "    final_score = best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final submission with the best approach\n",
    "final_submission = sample_submission.copy()\n",
    "final_submission['accident_risk'] = final_predictions\n",
    "\n",
    "# Save to file\n",
    "final_output_path = Path('../submission_final.csv')\n",
    "final_submission.to_csv(final_output_path, index=False)\n",
    "\n",
    "print('✓ Final submission file created successfully!')\n",
    "print(f'  Location: {final_output_path.resolve()}')\n",
    "print(f'  Final CV Score: {final_score:.5f}')\n",
    "print(f'\\nFirst few rows of final submission:')\n",
    "print(final_submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b8763",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Final Results:**\n",
    "- Ensemble model combining CatBoost, XGBoost, and LightGBM\n",
    "- 5-fold cross-validation\n",
    "- Enhanced feature engineering (30+ features)\n",
    "- Stacking with meta-features\n",
    "- Submission file created: `submission_final.csv`\n",
    "\n",
    "**Next Steps:**\n",
    "1. Upload submission to Kaggle\n",
    "2. Monitor leaderboard score\n",
    "3. If score > 0.05, iterate with:\n",
    "   - More feature engineering\n",
    "   - Hyperparameter tuning\n",
    "   - Advanced stacking techniques\n",
    "   - Additional models (Neural networks, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}